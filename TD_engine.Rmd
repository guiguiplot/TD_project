---
title: "R Notebook"
output: html_notebook
---

...

```{r}
rm(list=ls())
library(rvest)
library(ggplot2)

url_ajps <- "https://scholar.google.de/scholar?as_q=&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=&as_publication=American+Journal+of+Political+Science&as_ylo=2000&as_yhi=2017&btnG=&hl=en&as_sdt=0%2C5"

page_ajps <- read_html(url_ajps)

library(stringr)
citations <- page_ajps %>% 
  html_nodes (".gs_fl > a:nth-child(1)") %>% 
  html_text() %>%
  str_extract("\\d\\d\\d\\d\\d|\\d\\d\\d\\d|\\d\\d\\d|\\d\\d|\\d") 
  # str_extract("%04d")

# citations1 <- sprintf("%03d", citations)
citations


titles <- page_ajps %>% 
  html_nodes (".gs_rt") %>% 
  html_text()

links <- page_ajps %>% 
  html_nodes (".gs_fl > a:nth-child(1)") %>%
  html_attr("href")

author <- page_ajps %>% 
  html_nodes (".gs_a") %>%   html_text()



dt <- data.frame(titles, author, citations, links)
library(dplyr)
glimpse(dt)
head(dt)
```


```{r}

url_ajps <- "https://scholar.google.de/scholar?as_q=&as_epq=&as_oq=&as_eq=&as_occt=any&as_sauthors=&as_publication=American+Journal+of+Political+Science&as_ylo=2000&as_yhi=2017&btnG=&hl=en&as_sdt=0%2C5"
"https://scholar.google.de/scholar?start=10&hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0,5&as_ylo=2000&as_yhi=2017"
  library(stringr)
str_scrape_journal <- function(url){
  
  # read file
  page_ajps <- read_html(url)
  
  # extract amount citations
  citations <- page_ajps %>% 
    html_nodes (".gs_fl > a:nth-child(1)") %>% 
    html_text() %>%
    str_extract("\\d\\d\\d\\d\\d|\\d\\d\\d\\d|\\d\\d\\d|\\d\\d|\\d")   # citations1 <- sprintf("%03d", citations)
  
  # extract titles
  titles <- page_ajps %>% 
    html_nodes (".gs_rt") %>% 
    html_text()
  
  # ectract links
  links <- page_ajps %>% 
    html_nodes (".gs_fl > a:nth-child(1)") %>%
    html_attr("href")
  
  # ectract authors
  author <- page_ajps %>% 
    html_nodes (".gs_a") %>%   
    html_text()
  
  # gather data
  dt <- data.frame(titles, author, citations, links)
  
  return(dt)
}

dt <- str_scrape_journal(url_ajps)
glimpse(dt)
```



```{r}
s <- seq(00,1300, by = 10)
index_page <- sprintf("%02d", s)

links_page <- paste("https://scholar.google.de/scholar?start=", index_page, "&hl=en&as_publication=American+Journal+of+Political+Science&as_sdt=0,5&as_ylo=2000&as_yhi=2017", sep = "")

articles <- list()
pb <- txtProgressBar(min = 0, max = length(index_page), style = 3)
for(jj in seq_along(links_page)){
    tryCatch({ 
    articles[[jj]] <- str_scrape_journal(links_page[jj])
  }, error = function(e) {
    message("no file found!")
    message(e)
    cat('\n')
  }, finally = {
    setTxtProgressBar(pb, jj)
  })
}
close(pb)



str(articles)


library(data.table)
articles1 <- do.call(rbind, articles)
articles1
```

```
#Mein Ziel ist hier die Anzahl der Publikationen auf dem Economic Voting für jeden Jahr zu scrappen

library(stringr)
library(rvest)
library(dplyr)
library(httr)

#Um nicht als Bot annerkannt zu werden eine wichtige Etape ist der useragent zu ändern

#Neu Option Anstellung
#options("HTTPUserAgent"="Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.2117.157 Safari/537.36")
#Kontrollieren das Useragent richtig geändert wurde
#getOption("HTTPUserAgent")


#Links - generieren

#Anfangsdatum für jede suche
start_time <- 1950:2017
#Enddatum für jede suche (ich möchte jedes mal 1950:1950 ; 1951:1951 usw...)
end_time <- 1950:2017

#Linke Generieren
links_page <- paste('https://scholar.google.ca/scholar?q="economic+voting"&hl=fr&num=20&as_sdt=0&as_ylo=', start_time, '&as_yhi=', end_time, sep='')

url <- links_page[1]

#Function erstellen um einfach die Anzahl der Ergebnisse zu scrappen
str_scrape_number <- function(url){

  number_page <- read_html(url)
  
  number <- number_page %>%
    html_nodes("#gs_ab_md") %>%
    html_text() %>%
    str_extract("\\d\\d\\s\\d\\d\\d|\\d\\s\\d\\d\\d|\\d\\d\\d|\\d\\d|\\d|\\d\\h")
  
 
  
    return(number)
}


#Function um jede Seite zu scrappen
numbers <- c()
sleep_time <- seq(0,1, .01)

for(jj in 1:length(links_page)){
  numbers[jj] <- str_scrape_number(links_page[1])
  
  #Interval zwischen jeder Query
  Sys.sleep(sample(sleep_times,1))
  
  }

numbers

```
